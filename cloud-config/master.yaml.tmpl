#cloud-config
users:
  - name: joe
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDE178xsxTfHERTXpzxbd8AsH4l1kQ+2y2+s1Ed0YQTfbNzCHMBKuCmabyv56QISc0Frp6oFNutmbRQpRlxNRzvWvcdapb2+wNQIOpc6/aQBPbiyCdU6Tjcw1p3p7z/O8M9wIPZ2e9zYyUjV0EzN/iZxrdDBduF1mrAjKzeG9E+McEUaD3LIJCxmljrt3248wusHvdwpLJGTM8K8ajdrlKNET9KEI3lWTaHBxr8v/cPixBJb+rxnMZuBRV/Hc3XN13OhW3wVftGMkgjrS0oVTcXE8YlrCYCNNlw+A1hVHZ3XBbV/g1Ww65lmL2AOHrOlnUd96bbocFcm6btqUuWr1clDfEZ/FvfAvWKe9pZb2rCxqOCnLzZmB6zUPj9dS8Cg7nnXZFfsIE0p71sO2i4cYd0l9uzQpmsxYPAy+BAdRpR9P2oM1CM/DbLjlO5IIb9qTB3O4R2zaG5WpVjAdvqo9XptXKa5uIi8ZoVHvhCdnqskwaXsMpEHavQVvdxPBal01smXxFv6lLqKMVkzJRBkXBEWXvxa12pv2kiFnaxMWK95jqLFHXpjZVrYS1Z77ld9+SXGr0KjAvd6SShPg1ggiDAd4suBDUbeyVQyhzr0CGJ4auiqHsO5IDSdaFFo7xeqnBzAT+jxsBfzKhn9In7HZNf1XnG+2fF41eqnobWwMbCaQ==  joe
  - name: marian
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMSBIK7+MCULpHyUhLvYOKgQEyhkBR1n7/5oyKhzAD5pvAjqBzBcQ64S4EYxL0/Y/JDa2bMzAEzaw0VL1e2kz/nAfd7QiW3ZgyU6uYGDwHHWDekAY+Q30giQoqP3QxFSDTjUVb1EC4kIO49uzAwItwM2ah6C/Jmz4/EWMP+2MKrwCe8DUTCYPI0RyXpyj0O0Uz+11VGVCIdMbxq3O62giC4WwNUFC+RDGS4plrsOo4whrLOlE7ZrYjSp1dU+GdNQmrKXJA8j9k8asIsChljrx6wF2aS7gMF5ltj8M3ufk1Cz4FN8/5luAE0qx14I8K0yej8Ann4dohrRm8sPz3aQOh marian
  - name: puja
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDFKCoKe/Z2cT7duLiiPdgUXKcsHx+3ESCa0t4hOZtfw6BHptJ0dpDTAqbkvpGRwErVpOO9tIQittWnzgX0RLqnDB98I7eZQ04AIZwoW9AX2aMEyLgbVMTeG9Xgy79pefjVV75T2lVjXtcOY2wJtf6ZU1KFq8/dHZb7vYzzobHBq9j4vIB5ZsNI94jJm7I6TLr24vga3+MEgQrsEQdRqZ2vxacSU1h+LSdfseGQew1XlxSTfTfglUcXE0WUlEFnak9z0JwQbblEmKQsinIwO4O0Sk6FQXObCFlss//gubk64/OM/87I/aKjrmbQTRMkxyqJ5jO4yIXOxeHpp5kNA9AKSmgHABhr1ViS6ocWO8mMekbLdxDWMdViTR6XxtFSPUCgTFAirsQi6/9qfV+6u2RLhKihuajy8akFi4BYqSGr17/crrkCYydBJRUIqNmQSdzGKodTJ9d556iFZ3rCM+Xe2mm4KsHkIQ3YphPMzb0yAWEtZl1ncdqSXHz74M9b1KHUzyJgQhv5KOzhURxXR/UVBy7NNPae8XSEFId/O2uHgc/mWV5Xr5ZwxbwXsmlyto53+EmgynnPcL96RgVyiAmHL/vtvOGAzVSOPtNsnU7QG/YDfA2WrxLmuGEA0WzC63iXZCqSFbPK0adelJo9vctCB2gozrVpjqXWpskg8SZMqw== puja
  - name: teemow
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCvDv/5/bOgnuNXbxB7n8n8reDpRdAPeZvuCE3pJPVYJNfL0hoNzGhw4MBLRivSd9ViMM9X1CE7iVK6RFO46yt4GK5JWYi1UM88q6I+4bcnswUAWsIFX5a/U0LK0wIR3akDDyU9WEA7CXoJ7IxF7dtIYC9OIqrD6gXc4P/UI0jZQI5iZY3qNjlKVAwsNz8pD/BE2sPpNVHumzgcLJEveoc3WMCmfBAAWQAMfRlhlJ5LjM7Py/5k1/s5Myn4L/yoAvxMWev4k2ZYpumt752/r927K7AIrK/OYTfqLPKzZYSLWAj4g7L3/65sKpFm6g+HFgDmlScgf9P4bAoLn6+mWbYL teemow
  - name: tobias
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCS/thnoYaUYmuDxwQF9ES3Jsq6KltO+QU/8PVo1tUr5vlEfEY1Q9JYHiPKJW+U0cMH3a/Jv2IDTaH629djoNdtTottaYGDINBoVIlAdR+vwm3JzVUB02mb+QXTzhzLc58fdwhHN0PS82/BcSSFpQzI7PedRGMtzS6Qxcx4YfrzC16vsdF8wMw+tVbtI2fDLwfd9NcpCDr582NrX/qOR22zeck3VVgppnuC5mGAC+XvHCRbp+4pZJ0W4fpEIGwG1cPbktvdA0wYcn7GJo7fU11066PMGMXplV+DEnQTpBUbP+NFXRuY7RzTeuTGSZHXsWO11cmpLPVVB7LdAaQuSPi1 tobias
  - name: xh3b4sd
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQClCCgsKl7+mQwD+giN6OEruV1ur/prpWXfyGHJyGGQkROZA3IcrpmRPWmKKXpCaW+G8lcb9DXD/K7/rNAh+4hpsfvCUs8u0mJ6u4El/8dcRTQaZUdLX8q3AZZ38gmk+yZz241x7LGd05D4H+aq9sVdtbcAepINUJyZ7p3yXTfCYwHC7QMYiuRFKMaUHY50shFhSYdD9TCEFtH2ybPi1/WOCX6gf90f6O0Ivo7tzwtYGV8ToIa2nO+CqwlIRiGqEy4/g9h1gCPDvgcLZmok74V6mH12whNdMDyJyuT8S1dLwNiKoYkvMbcUkpE0O/0LBCg+SsHVHmgnsNx9t0hUg8iR xh3b4sd
  - name: calvix
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC9IyAZvlEL7lrxDghpqWjs/z/q4E0OtEbmKW9oD0zhYfyHIaX33YYoj3iC7oEd6OEvY4+L4awjRZ2FrXerN/tTg9t1zrW7f7Tah/SnS9XYY9zyo4uzuq1Pa6spOkjpcjtXbQwdQSATD0eeLraBWWVBDIg1COAMsAhveP04UaXAKGSQst6df007dIS5pmcATASNNBc9zzBmJgFwPDLwVviYqoqcYTASka4fSQhQ+fSj9zO1pgrCvvsmA/QeHz2Cn5uFzjh8ftqkM10sjiYibknsBuvVKZ2KpeTY6XoTOT0d9YWoJpfqAEE00+RmYLqDTQGWm5pRuZSc9vbnnH2MiEKf calvix
  - name: r7vme
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDntH7rGzQxvcxxOxsEP3scm+2e/hxKAoNjeilckuLbklMKRt+LR1M+sSKLJWgIbFTg0GITqnugk+7jPP2NbRsEiuYN7Scy0iRPWdRMbVjGbtMBHBshPp4J+uUS8n5YquMA2xA9cUa7K+VTo+Plv8pVx0LwgE7Rw5NQFqegaiClZJZapOw+LZkF/Kdrm9DAkuCH3jD543O76xx45dAIQZLWlLYNYeBdYT8ew1ewiSn8mY1F7rhLcjuoBGH1a2Z4iR3vgP36yqFDcrqD9gSG/5xupdKbss5vV4acMeTJu/DKoHCDj9GT5//PKVYjOnNlhEiQKO6Pft/aBgk8ND1ixY5 roman
  - name: rossf7
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCY0Rn2VqhtOFy7LY6nu53c9bP5Fy0KQ+P5/MA/s4aG8+veIqTyhhpHLwPF0hKbi1mq+HaFvm1nLbovZcTTG2Z+BoP/Y5kV5EOjnq415EtT3rH0YdM0h69Qxuc0KqUvU/F43XOhpEH0o8L+ZK+Vq4UrRPIDRjftc8N5h6MJszAow/kiC7d30nYsPio6FuWHH5jZaAKjucQbBsDU5r160mtVk0HCexutm3s3fHTADojZjFA3t8FJy7vO+Og+VDVzV9ai9E32mgytNL0wVE1dUGqPwoM9MrzxNC2TZedS74zqBoK9TL3y1sfVzD5RpdX4Z5FInhtTz1z4nnYzsPiYQKMx rossf7
  - name: oponder
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAxEDA/Nr7++9SoFOhTeNJMAubkzJmGZWHtXul0kM+FJR0TM/1V3b4XfoRAwU9gaG88P+4venSwcbLVfvvacnlGQ8hTgw2Jlpz6Z9+iIVjru2+nYgJeELFff5bdPLYWE4Ft/VYpwGibD1DbVGldsO3I7EdaEfd5FeOF0Fk1xPK50UGvTq9CkU+wEcTc9eDzFWpLoz/69KWG/F7XEZhWqswUjHaN1UJtuBlVmoe/0OrlyIYBl2CeUzpmJHNDDv7u8gKOCOFwmzMdDieGHzIITCovIoGLhIJS1dGdT3FPvTPG/s8VOZ/QGS9rfsc5x08J0v0JqwN8GZT9FxFJECeXTN6bw== oponder
  - name: kopiczko
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDzY+7kp4XTNfinVLbDo28E0yaJUMvEabzdsheGHG3+gubakJgITZw+3m4vy45WRoF8QDOjpZ12n9ov9bpz5X8kRmHSthvWYtNRYJWCJc0d3+td1/Ki9CaHesNhKdeVYcw9g5x55h6o4EPx+g6wIsBhxqjcdZ5O37M+KWXlBfLoP4WKBjORhD4kpU+suA+rMIRF/njLs8zswHL8or3Ynp+voZM1PVCfxENp9ktNeA7W2KyUZpgDtoWxN2cnj0BOs/t2w+XZhqgsPo/9zXO6C0XIvPv69MAOHYMsomKldQgpy+MlODvu/sbP5ruB/4vGiqCg0+E2KS1ZK85KytQtfai3IcmglNtHdRKyYmf2WkY/GMGB6sgQKZWJE4XeN7+mZvRrOih2yo+/GJCkI5U0eWIUInyh/lBMIxVbckbpdZYxMd2SaERkTFNDhqSgncrPu5gW7ZFejgie9gVlkYBLI/D1XXM1YzvpSlA80D9kxLhiqdaAXP7CysPgS7EN56zM0SzHR0vxrr4dhB9XuBxlMeTtC0dvaMPkiIJj43MLDNzE4wtXEpzFQnmzRoOrPAACRhr0OcOTbsH7X+QR9JUB1ygTRSN/kHVFyL+EPRRsfuzbxw57jkzIE7ihGtcINtyv4dIFfqE7c+uDDZV/yUi2C5PI6FLHW33DXwqbzR8yZuOu2Q== pawel
  - name: marcelmue
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC9pOt/FFXonkNDGamoGMVg6wQJYS7m9r/OO2wWoEPNQS/M4nGL/szKlZfD4Z6tKS3WyeLY5XU7QjOFb3Gt3QQAaVOSDgTkfH1i+usWSNzFlhjARQkIUs0j9m30o7sXznZNOy4r73bnfUjTwYJifGUWBPq/jGokLNCxBRPCaJFi8Od5De3DyuDd93SAoXkTJaDPr7J90tzkVLI6ek5va1GSTeVdHbAifds8r8Shm0wgdmcVKiOYt7/oyzavl0x5XPzMAVXeUI0jIopsvqjiy/fS+Cq7i1TMBQ+rkycWLc8X8CM8U84OQ6eb8LgQw0A4xqVtZHOl2FlHHtWNjLhnwO5MHWjdWxCUSEshK+tS5Wm64ph37nfObupPpoMcRRTmKB2SdsigU0T/aJ9zoJorsIBKDY6lqmXoYk07XEyu5tmuG9cuiP900yLRIoCZeQ14WP3+3KDWLfjic3W8wXr0xlLOTaPNtNpX+v6X4n8R4HuJ0zQw1znEXhBlhTEZTRe1qmdPQTNwQa6XwOlEJVYOIHEOobTQVu+ReIrj+XT3b1VR531pG8M3o9Rhq29aFpAsYiDh12aGk45b+61hddF3WybpMOVQqfvYnf1lVwt/0PujJuIC7e6WuHAlRS2Sshb19ROG8w2mW1sGbp2zN7Y+MUAI5LVOHrBtRqEoZkhz+JXMzQ== marcelmue
  - name: vol
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDFGDg/p4JWewXAs8kExJnCaNXEN1v2LZf0YWWiblHFp1+i2bp8qSmAJT3i6Yw0kHY2/6MotBCKAsFtlqxuhKaFs3jDcmdOugmWz4Qj7oerQ/ypJE/wZ9PY79gbK75aEKyOdVf7dUT6Ah+oSfETgpY/3a9pVZ/dSF3WBFIBw5k4YarFzcELQE4Bo4dcsLHsNrkI9Bk6gkGbTY+1TtfJmOu0bEXxXHdEq+JfW0MFssjh3I5n0DT09qDnztAvRAjjqjlyNKNt8reErV0LlvsDM5c+426Bz9JgM5vP3sD5ai8lpuH0iCBHoo9678XTKKTYbbz0s7kgXUb0vGS+GbOcaKBKmZ8a0xDpsft9+/LbmnuUic8b4c4/cRw5wSV1IYqyDqARp/d9PaJlYa22ISGnDbYmXUTsef0PhUenK9gtYrGsVhQmkqeLYiIYqwsl7+uouFMpQDmdZjY/B4fKcRA3oRGCFuwzT1vrtJL41dw9WyzM+3xnHTMFZdko9TlgDiEeu6gdpsTGJf4VALUWgXeyW/egte2im86kjMxzQuCw/aOmiYMqwZH2YfI0dS9jLuZbxePKTUounct66SrNXBrbu2d0BiPj6bl1dG6oZhwtArRnbiG5+cTakDvLhFgahTQFAT1De7o3Nr+BfjNQkVlQNKaIPUOdypiDNJE/6q/GOHVRQw==
  - name: fgimenez
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAxnaIJtNvjR0sPO92hJla/Q71pczHn9KFx/Tc4gHHgY8kY6ysV7F9JxcfmDPzpTrYPNAylHTUR3SNWZGcgk2hYo2AChbTNyUh+PiyrvsEenMifl8e4qnb7145BStK6QdxQkZwkhVebm+T1pXEO+RwR9x2SPdV8+vHJ8EZ/FHwMyQAKGFWXYHkPCtwMgoFElB406VDeGxXCpeL1kpYGpK/2CgXgz4Dpm8nwvxfOWAmBYIDKOIrblzraQhFiyqBQp0fNlI2lgxFEGxpaMLL3WguMGqf7kKAvupUz32DWbY9i6//xFkhoaMWWiigAOZSWCcq7yE30d63Pg3mZE8YMIw3Kw==
  - name: jgsqware
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDg0b+m551vLRqsnnDrB32PrQnO41Fv62dbYYCGqjcd/if8iOuyXvxNRnbOxnsFFPVSiqY2LB8pXofEAm5MG2qupGyBvivtnq1v+6Ld9nMYTaKdz5WKhI9ypQ/jV4G1DNYrayGno13eRmGemCEnIdZeRrVxp5EfkVX0ZyJ88998Urjv6OtSLV+GSNSiIbNYyvGjLoR0dt5LCVbwbbQd1H5wXYsSoeIkKiqfS7AtMn9wDCIyM1W15yC/4UaCMEGkVfjLZB+4Y8BBfLH1vI1h43zl4EUkaq1PASDvpX0AWlclfemkK2bGEOS/UzVJsZtM73jEoSZvq/aCLe3v/0zI/5Xl
  - name: fernando
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDdprdoupVo8EH7yL3BDwj1odnJfVs6ab0BZehepcVGL7Yno5YhTM/cof/KvGWZyzER1m0hn8adm9j6HqjhskzhoUSq37b0q4v38t1oZZ7YPjF5b90bvQKnUv/5U6xqOFnE9LfotLFT8cidLLwPSDKOlPacEmSheDKTRFZwAfCJuv4eobZ3Hs0qCfzx7IjKqxYWfEJSvp9x2EUHBJnBSZ3n1ncbpyI6wyyXgjoo9v7bXhWu+HRB+LyXCPvDME7ZWihNdlX4ZesiEoyOtoVKZsBx4/L1ckyhPN2NNDNElG41/w8znrbnKBKPiBM5AGnc7XZw2TP1ivWhJ2EVBjvtPT73GMgR6AdIV4iInNdg89mLKNtHwqZ8Gyj7/xS7ufdMFST1EGnBnk/mGvK1eITWVv6Y9s5Zte84LRhEK7W2jQtND70lT3FMBKLVO/W0q+suWZIqAYWzfpASnvaXlLWIrRApzslzgbyP14Zdp1cTzjwXOoQ+FhHRlqhX7G8uGR7JvNaS732kD8xASDzZulDrkmtxDVYqKFTxRQRwolmCeDzokS2OdnRncS7m6AxwMbwI0QuQwQZp3+G0qIxKXCad+XPCQt2oaYJJRZD+dqPJOKSofbS3eZayP0nPVNqXBP55S4H2PGGSATIpSRQdfy0bebV54x7N+P3KBjxzvIpzX0nn4Q== fernando@giantswarm.io
  - name: theo
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDo5PU8w8RDWYDd8SSIKOiYJCeN398PAEJApFGpPWprewBmiDGGMHHDIDV//QR+o5MhR2hBmJ+Pw1K/Edr4u0cfGlIIb6lSVdha+jDEp0l1PtqyQzubzTH3y/RDzxCakAa419N1G3DwzJHkWBxbpyqx7i/DOYcxgQP4vCGvYgkvuOkQYCNk2hfuXAl/Aucv3JXlsuNktyXQ6XKf+Twa2Bg8jIAaYUNGqgKgzMcsCElE55bxVuYeXl441CzD2fdHmXyGwo6nefN7PZ790SxQzkGM8wBpESgc7U4IPUY/dnsn4yQBYw2meontHWGLmZjrvEYxoS7Uv4o8BX8cScgVZUhRojHvNWBBcwOx+hhuPbqoqdc8IFXQLHTTa/szvY9gwlipBejj6nJrRpl3Kxw+EX4QP/loDxkykWQUoByU479Z6/gOtgAkPOe8xZblny6r3uCZyUlaYR9ht2aOEbH8bLuYBaDTPvunMIH/RSgbNxys/Dss3ZC7MJgXtoaSpb/AGdqv1Uj4AdNJJm8544AFhmR/Tky0rms3NpaSEiwO+E5ZIJiBevqPbGWkodbfKM6uydS+wrqHOR+zTNLuwhTHVnNRZ//ePBMptnzR1qbuMmaEgmqMM1HjFflUUVdDLFK6TxcdU3YPJnnWlwGk2bCjlamGjHx0hvoJVSY+Oap6o23cJQ== theo@giantswarm.io
  - name: tobi
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCzkt5Tls3jGFzHW/cj6kz3OPAYR+fINFlDFk7qEY98sjkAbKqTwntVLY7ghOdynmt+0qoQRHklLoxDTUsFoYUu2vOpdtKKCdzHR08uYS2RlvOdA+Ex0d0qEHyxX4H3b+ym+8BqCn9Bh5X4/thGWt3cqVtroqYeUAES5SneQ1rX420hN9MoZ+83vC5gwTr90GqZbsqweNTHPkhuYLylvphzxbXOitzLr61VRxjGb4gZiRD6kCXuW/uP0nrtYXkHjROXMvdj1ZZA+VLR4TAbIukApYa3JpRAApanNScgmZTsa/HhEfDVQtGesG+C6fsZVcStxuWMcpx1vZFZ6gkeIDw+R47250E3BzaDYhjGo6URuijcmepllHsU+KgpvawmgjN8hlLdbBfCDLNML23mPDIwZmUznVwL1qH0Vf744ioExKKLHx1RpiohQMCnE6K+x85tf/5NAy/VlNPxD4qeS62Bcr4DPNnn3My1h+1U1hDyclw/4SynAZXNlBh7q6ksjzaRZ68LsZhWsyOZMKuz+igkV509SYhjqZfCT34zRlxEmRaI05fVAst/mZOfsfWb1AhADNVNF172jfat0qeot/Hs/Fs4D6bpl0mwW2jQD3dB3OkytroM4TBSvG7weJwJWoDgLvy1gx6Nd4rIsUlnQKivzwmEgCaV448Y6sVpEWpCAw== tobiasrichter@giantswarm.io
  - name: tuommaki
    groups:
      - "sudo"
      - "docker"
      - "rkt"
    ssh-authorized_keys:
      - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCu2V1p0cZ2N4ucug9LQMB+YMg9AQ5+aZQTdDTZ7oBuEcBuGtdnSSbcxj1lHoMYvhz6ugFVolkusRnZSakZY/XPVlwIHC56TWWrJ0hJ4sQEzCqVSHx0ZBHaMZepxCz7KSh/4KjtZFyaBC9SFwUo7kGgBYoFdClhxZsmfMsk0RneY8FjWme/cwXSaEGdaaTyOA52UOCg6Ax3nnE/gAJBsL8HgI17bFjj8og6TdPoP+33wujGHFORy8HF/m6p1I2Nm9Mp+gkG6PzdkWbF7UFci5uYHXy5IEu6uGzEPQiB5BjgfVIvZyH3VfKxmG1T2yyp4/qDQOmkjlIahpPyI00Y3SWAab7MdQXJ2hTgWFo/NP+AEdd45+PrSvTMy2k5bVl9GMntP+z+9oAhwH8OStSCJ0GBGlVG89fd0vFV1XVmLPwS8XhuhAoU1KRt6/Hc8cs7uSUiKOTY8Xn6VNUozxK137QpHBb81jU7OCcmopF9dlqoV6m18iZK1NjP4+FFxUyi5O4HI6aFrZXf7Cw5G9C8EXML3qLIMxd2pIJsu8QTw/5kC7sBtmFY/5RqW0TZ5hWuyGSuFcRan5E08Qct5rGAQ6QjJ9rZqQUPeJFcN6gEvGUam0XdeziZD6lPFUDkte9y653lIrPqBoSbsJuk/FJU/+RTSYEl+VCmaac3ru6jYV6M8w== tuomas@giantswarm.io
write_files:
- path: /srv/calico-all.yaml
  owner: root
  permissions: 644
  content: |
    #
    # Source: https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubernetes-datastore/policy-only/1.7/calico.yaml
    #
    # Changed values:
    #   - CALICO_IPV4POOL_IPIP = off
    #   - CALICO_IPV4POOL_CIDR = parameterized
    #
    # Calico Version v3.0.2
    # https://docs.projectcalico.org/v3.0/releases#v3.0.2
    # This manifest includes the following component versions:
    #   calico/node:v3.0.2
    #   calico/cni:v2.0.0

    # This ConfigMap is used to configure a self-hosted Calico installation.
    kind: ConfigMap
    apiVersion: v1
    metadata:
      name: calico-config
      namespace: kube-system
    data:
      # To enable Typha, set this to "calico-typha" *and* set a non-zero value for Typha replicas
      # below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is
      # essential.
      typha_service_name: "none"
      # The CNI network configuration to install on each node.
      cni_network_config: |-
        {
          "name": "k8s-pod-network",
          "cniVersion": "0.3.0",
          "plugins": [
            {
              "type": "calico",
              "log_level": "info",
              "datastore_type": "kubernetes",
              "nodename": "__KUBERNETES_NODE_NAME__",
              "mtu": 1500,
              "ipam": {
                  "type": "host-local",
                  "subnet": "usePodCidr"
              },
              "policy": {
                  "type": "k8s",
                  "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
              },
              "kubernetes": {
                  "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                  "kubeconfig": "__KUBECONFIG_FILEPATH__"
              }
            },
            {
              "type": "portmap",
              "snat": true,
              "capabilities": {"portMappings": true}
            }
          ]
        }

    ---

    # This manifest creates a Service, which will be backed by Calico's Typha daemon.
    # Typha sits in between Felix and the API server, reducing Calico's load on the API server.

    apiVersion: v1
    kind: Service
    metadata:
      name: calico-typha
      namespace: kube-system
      labels:
        k8s-app: calico-typha
    spec:
      ports:
        - port: 5473
          protocol: TCP
          targetPort: calico-typha
          name: calico-typha
      selector:
        k8s-app: calico-typha

    ---

    # This manifest creates a Deployment of Typha to back the above service.

    apiVersion: apps/v1beta1
    kind: Deployment
    metadata:
      name: calico-typha
      namespace: kube-system
      labels:
        k8s-app: calico-typha
    spec:
      # Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the
      # typha_service_name variable in the calico-config ConfigMap above.
      #
      # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential
      # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In
      # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.
      replicas: 0
      revisionHistoryLimit: 2
      template:
        metadata:
          labels:
            k8s-app: calico-typha
          annotations:
            # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
            # add-on, ensuring it gets priority scheduling and that its resources are reserved
            # if it ever gets evicted.
            scheduler.alpha.kubernetes.io/critical-pod: ''
        spec:
          tolerations:
          - key: CriticalAddonsOnly
            operator: Exists
          # Since Calico can't network a pod until Typha is up, we need to run Typha itself
          # as a host-networked pod.
          hostNetwork: true
          serviceAccountName: calico-node
          containers:
          - image: quay.io/calico/typha:v0.6.0
            name: calico-typha
            ports:
            - containerPort: 5473
              name: calico-typha
              protocol: TCP
            env:
              # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
              - name: TYPHA_LOGSEVERITYSCREEN
                value: "info"
              # Disable logging to file and syslog since those don't make sense in Kubernetes.
              - name: TYPHA_LOGFILEPATH
                value: "none"
              - name: TYPHA_LOGSEVERITYSYS
                value: "none"
              # Monitor the Kubernetes API to find the number of running instances and rebalance
              # connections.
              - name: TYPHA_CONNECTIONREBALANCINGMODE
                value: "kubernetes"
              - name: TYPHA_DATASTORETYPE
                value: "kubernetes"
              - name: TYPHA_HEALTHENABLED
                value: "true"
              # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
              # this opens a port on the host, which may need to be secured.
              #- name: TYPHA_PROMETHEUSMETRICSENABLED
              #  value: "true"
              #- name: TYPHA_PROMETHEUSMETRICSPORT
              #  value: "9093"
            livenessProbe:
              httpGet:
                path: /liveness
                port: 9098
              periodSeconds: 30
              initialDelaySeconds: 30
            readinessProbe:
              httpGet:
                path: /readiness
                port: 9098
              periodSeconds: 10

    ---

    # This manifest installs the calico/node container, as well
    # as the Calico CNI plugins and network config on
    # each master and worker node in a Kubernetes cluster.
    kind: DaemonSet
    apiVersion: extensions/v1beta1
    metadata:
      name: calico-node
      namespace: kube-system
      labels:
        k8s-app: calico-node
    spec:
      selector:
        matchLabels:
          k8s-app: calico-node
      updateStrategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
      template:
        metadata:
          labels:
            k8s-app: calico-node
          annotations:
            # This, along with the CriticalAddonsOnly toleration below,
            # marks the pod as a critical add-on, ensuring it gets
            # priority scheduling and that its resources are reserved
            # if it ever gets evicted.
            scheduler.alpha.kubernetes.io/critical-pod: ''
        spec:
          hostNetwork: true
          serviceAccountName: calico-node
          tolerations:
            # Allow the pod to run on the master.  This is required for
            # the master to communicate with pods.
            - key: node-role.kubernetes.io/master
              effect: NoSchedule
            # Mark the pod as a critical add-on for rescheduling.
            - key: CriticalAddonsOnly
              operator: Exists
          # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
          # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
          terminationGracePeriodSeconds: 0
          containers:
            # Runs calico/node container on each Kubernetes node.  This
            # container programs network policy and routes on each
            # host.
            - name: calico-node
              image: quay.io/calico/node:v3.0.2
              env:
                # Use Kubernetes API as the backing datastore.
                - name: DATASTORE_TYPE
                  value: "kubernetes"
                # Enable felix info logging.
                - name: FELIX_LOGSEVERITYSCREEN
                  value: "info"
                # Don't enable BGP.
                - name: CALICO_NETWORKING_BACKEND
                  value: "none"
                # Cluster type to identify the deployment type
                - name: CLUSTER_TYPE
                  value: "k8s"
                # Disable file logging so `kubectl logs` works.
                - name: CALICO_DISABLE_FILE_LOGGING
                  value: "true"
                # Set Felix endpoint to host default action to ACCEPT.
                - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                  value: "ACCEPT"
                # Disable IPV6 on Kubernetes.
                - name: FELIX_IPV6SUPPORT
                  value: "false"
                # Wait for the datastore.
                - name: WAIT_FOR_DATASTORE
                  value: "true"
                # The Calico IPv4 pool to use.  This should match `--cluster-cidr`
                - name: CALICO_IPV4POOL_CIDR
                  value: "${POD_CIDR}"
                # Enable IPIP
                - name: CALICO_IPV4POOL_IPIP
                  value: "off"
                # Typha support: controlled by the ConfigMap.
                - name: FELIX_TYPHAK8SSERVICENAME
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: typha_service_name
                # Set based on the k8s node name.
                - name: NODENAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                # No IP address needed.
                - name: IP
                  value: ""
                - name: FELIX_HEALTHENABLED
                  value: "true"
              securityContext:
                privileged: true
              resources:
                requests:
                  cpu: 250m
              livenessProbe:
                httpGet:
                  path: /liveness
                  port: 9099
                periodSeconds: 10
                initialDelaySeconds: 10
                failureThreshold: 6
              readinessProbe:
                httpGet:
                  path: /readiness
                  port: 9099
                periodSeconds: 10
              volumeMounts:
                - mountPath: /lib/modules
                  name: lib-modules
                  readOnly: true
                - mountPath: /var/run/calico
                  name: var-run-calico
                  readOnly: false
            # This container installs the Calico CNI binaries
            # and CNI network config file on each node.
            - name: install-cni
              image: quay.io/calico/cni:v2.0.0
              command: ["/install-cni.sh"]
              env:
                # Name of the CNI config file to create.
                - name: CNI_CONF_NAME
                  value: "10-calico.conflist"
                # The CNI network config to install on each node.
                - name: CNI_NETWORK_CONFIG
                  valueFrom:
                    configMapKeyRef:
                      name: calico-config
                      key: cni_network_config
                # Set the hostname based on the k8s node name.
                - name: KUBERNETES_NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
              volumeMounts:
                - mountPath: /host/opt/cni/bin
                  name: cni-bin-dir
                - mountPath: /host/etc/cni/net.d
                  name: cni-net-dir
          volumes:
            # Used by calico/node.
            - name: lib-modules
              hostPath:
                path: /lib/modules
            - name: var-run-calico
              hostPath:
                path: /var/run/calico
            # Used to install CNI.
            - name: cni-bin-dir
              hostPath:
                path: /opt/cni/bin
            - name: cni-net-dir
              hostPath:
                path: /etc/cni/net.d

    # Create all the CustomResourceDefinitions needed for
    # Calico policy-only mode.
    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico Felix Configuration
    kind: CustomResourceDefinition
    metadata:
       name: felixconfigurations.crd.projectcalico.org
    spec:
      scope: Cluster
      group: crd.projectcalico.org
      version: v1
      names:
        kind: FelixConfiguration
        plural: felixconfigurations
        singular: felixconfiguration

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico BGP Configuration
    kind: CustomResourceDefinition
    metadata:
      name: bgpconfigurations.crd.projectcalico.org
    spec:
      scope: Cluster
      group: crd.projectcalico.org
      version: v1
      names:
        kind: BGPConfiguration
        plural: bgpconfigurations
        singular: bgpconfiguration

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico IP Pools
    kind: CustomResourceDefinition
    metadata:
      name: ippools.crd.projectcalico.org
    spec:
      scope: Cluster
      group: crd.projectcalico.org
      version: v1
      names:
        kind: IPPool
        plural: ippools
        singular: ippool

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico Cluster Information
    kind: CustomResourceDefinition
    metadata:
      name: clusterinformations.crd.projectcalico.org
    spec:
      scope: Cluster
      group: crd.projectcalico.org
      version: v1
      names:
        kind: ClusterInformation
        plural: clusterinformations
        singular: clusterinformation

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico Global Network Policies
    kind: CustomResourceDefinition
    metadata:
      name: globalnetworkpolicies.crd.projectcalico.org
    spec:
      scope: Cluster
      group: crd.projectcalico.org
      version: v1
      names:
        kind: GlobalNetworkPolicy
        plural: globalnetworkpolicies
        singular: globalnetworkpolicy

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    description: Calico Network Policies
    kind: CustomResourceDefinition
    metadata:
      name: networkpolicies.crd.projectcalico.org
    spec:
      scope: Namespaced
      group: crd.projectcalico.org
      version: v1
      names:
        kind: NetworkPolicy
        plural: networkpolicies
        singular: networkpolicy

    ---

    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: calico-node
      namespace: kube-system
- path: /srv/default-storage-class.yaml
  owner: root
  permissions: 644
  content: |
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
      name: default
      annotations:
        storageclass.beta.kubernetes.io/is-default-class: "true"
      labels:
        kubernetes.io/cluster-service: "true"
    provisioner: kubernetes.io/azure-disk
    parameters:
      kind: Managed
      storageaccounttype: Premium_LRS
    allowVolumeExpansion: true
    ---
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
      name: managed-premium
      annotations:
      labels:
        kubernetes.io/cluster-service: "true"
    provisioner: kubernetes.io/azure-disk
    parameters:
      kind: Managed
      storageaccounttype: Premium_LRS
    allowVolumeExpansion: true
    ---
    apiVersion: storage.k8s.io/v1beta1
    kind: StorageClass
    metadata:
      name: managed-standard
      annotations:
      labels:
        kubernetes.io/cluster-service: "true"
    provisioner: kubernetes.io/azure-disk
    parameters:
      kind: Managed
      storageaccounttype: Standard_LRS
    allowVolumeExpansion: true
- path: /srv/coredns-all.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Service
    metadata:
      name: coredns
      namespace: kube-system
      labels:
        k8s-app: coredns
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: "CoreDNS"
    spec:
      selector:
        k8s-app: coredns
      clusterIP: ${K8S_DNS_IP}
      ports:
      - name: dns
        port: 53
        protocol: UDP
      - name: dns-tcp
        port: 53
        protocol: TCP
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: coredns
      namespace: kube-system
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: coredns
      namespace: kube-system
    data:
      Corefile: |
        .:53 {
            errors
            health
            kubernetes cluster.local ${K8S_SERVICE_CIDR} ${POD_CIDR} {
              pods insecure
              upstream
              fallthrough in-addr.arpa ip6.arpa
            }
            prometheus :9153
            proxy . /etc/resolv.conf
            cache 30
        }
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: coredns
      namespace: kube-system
      labels:
        k8s-app: coredns
        kubernetes.io/name: "CoreDNS"
    spec:
      replicas: 3
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
      selector:
        matchLabels:
          k8s-app: coredns
      template:
        metadata:
          labels:
            k8s-app: coredns
        spec:
          serviceAccountName: coredns
          tolerations:
            - key: "CriticalAddonsOnly"
              operator: "Exists"
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: k8s-app
                      operator: In
                      values:
                      - coredns
                  topologyKey: kubernetes.io/hostname
          containers:
          - name: coredns
            image: coredns/coredns:1.1.1
            imagePullPolicy: IfNotPresent
            args: [ "-conf", "/etc/coredns/Corefile" ]
            volumeMounts:
            - name: config-volume
              mountPath: /etc/coredns
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            livenessProbe:
              httpGet:
                path: /health
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 60
              timeoutSeconds: 5
              successThreshold: 1
              failureThreshold: 5
          dnsPolicy: Default
          volumes:
            - name: config-volume
              configMap:
                name: coredns
                items:
                - key: Corefile
                  path: Corefile
- path: /srv/default-backend-dep.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: default-http-backend
      namespace: kube-system
      labels:
        k8s-app: default-http-backend
    spec:
      replicas: 2
      template:
        metadata:
          labels:
            k8s-app: default-http-backend
        spec:
          containers:
          - name: default-http-backend
            image: gcr.io/google_containers/defaultbackend:1.0
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
                scheme: HTTP
              initialDelaySeconds: 30
              timeoutSeconds: 5
            ports:
            - containerPort: 8080
            resources:
              limits:
                cpu: 10m
                memory: 20Mi
              requests:
                cpu: 10m
                memory: 20Mi
- path: /srv/default-backend-svc.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Service
    metadata:
      name: default-http-backend
      namespace: kube-system
      labels:
        k8s-app: default-http-backend
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 8080
      selector:
        k8s-app: default-http-backend
- path: /srv/ingress-controller-cm.yaml
  owner: root
  permissions: 0644
  content: |
    kind: ConfigMap
    apiVersion: v1
    metadata:
      name: ingress-nginx
      labels:
        k8s-addon: ingress-nginx.addons.k8s.io
    data:
      variables-hash-bucket-size: "128"
      server-name-hash-bucket-size: "1024"
      server-name-hash-max-size: "1024"
- path: /srv/ingress-controller-sa.yaml
  owner: root
  permissions: 644
  content: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: nginx-ingress-controller
      namespace: kube-system
- path: /srv/ingress-controller-dep.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: nginx-ingress-controller
      namespace: kube-system
      labels:
        k8s-app: nginx-ingress-controller
      annotations:
        prometheus.io/port: '10254'
        prometheus.io/scrape: 'true'
    spec:
      replicas: 3
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
      template:
        metadata:
          labels:
            k8s-app: nginx-ingress-controller
        spec:
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: k8s-app
                        operator: In
                        values:
                        - nginx-ingress-controller
                  topologyKey: kubernetes.io/hostname
          serviceAccountName: nginx-ingress-controller
          containers:
          - name: nginx-ingress-controller
            image: quay.io/giantswarm/nginx-ingress-controller:0.11.0
            args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --configmap=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
            env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
            readinessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              timeoutSeconds: 1
            ports:
            - containerPort: 80
              hostPort: 80
            - containerPort: 443
              hostPort: 443
- path: /srv/ingress-controller-svc.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx-ingress-controller
      namespace: kube-system
      labels:
        k8s-app: nginx-ingress-controller
    spec:
      type: NodePort
      ports:
      - name: http
        port: 80
        nodePort: 30010
        protocol: TCP
        targetPort: 80
      - name: https
        port: 443
        nodePort: 30011
        protocol: TCP
        targetPort: 443
      selector:
        k8s-app: nginx-ingress-controller
- path: /srv/kube-lego-cm.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: kube-lego
      namespace: kube-system
    data:
      lego.email: "accounts@giantswarm.io"
      lego.url: "https://acme-v01.api.letsencrypt.org/directory"
- path: /srv/kube-lego-sa.yaml
  owner: root
  permissions: 644
  content: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: kube-lego
      namespace: kube-system
- path: /srv/kube-lego-dep.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: kube-lego
      namespace: kube-system
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            app: kube-lego
        spec:
          serviceAccountName: kube-lego
          containers:
          - name: kube-lego
            image: jetstack/kube-lego:0.1.5
            imagePullPolicy: Always
            ports:
            - containerPort: 8080
            env:
            - name: LEGO_EMAIL
              valueFrom:
                configMapKeyRef:
                  name: kube-lego
                  key: lego.email
            - name: LEGO_URL
              valueFrom:
                configMapKeyRef:
                  name: kube-lego
                  key: lego.url
            - name: LEGO_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: LEGO_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            readinessProbe:
              httpGet:
                path: /healthz
                port: 8080
              initialDelaySeconds: 5
              timeoutSeconds: 1
- path: /srv/rbac-roles.yaml
  owner: root
  permissions: 0644
  content: |
    ## Calico
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: calico-node
      namespace: kube-system
    rules:
      - apiGroups: [""]
        resources:
          - namespaces
        verbs:
          - get
          - list
          - watch
      - apiGroups: [""]
        resources:
          - pods/status
        verbs:
          - update
      - apiGroups: [""]
        resources:
          - pods
        verbs:
          - get
          - list
          - watch
          - patch
      - apiGroups: [""]
        resources:
          - services
        verbs:
          - get
      - apiGroups: [""]
        resources:
          - endpoints
        verbs:
          - get
      - apiGroups: [""]
        resources:
          - nodes
        verbs:
          - get
          - list
          - update
          - watch
      - apiGroups: ["extensions"]
        resources:
          - networkpolicies
        verbs:
          - get
          - list
          - watch
      - apiGroups: ["crd.projectcalico.org"]
        resources:
          - globalfelixconfigs
          - felixconfigurations
          - bgppeers
          - globalbgpconfigs
          - bgpconfigurations
          - ippools
          - globalnetworkpolicies
          - networkpolicies
          - clusterinformations
        verbs:
          - create
          - get
          - list
          - update
          - watch
    ---
    ## CoreDNS
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    metadata:
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:coredns
    rules:
    - apiGroups:
      - ""
      resources:
      - endpoints
      - services
      - pods
      - namespaces
      verbs:
      - list
      - watch
    ---
    ## IC
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    metadata:
      name: nginx-ingress-controller
      namespace: kube-system
    rules:
      - apiGroups:
          - ""
        resources:
          - configmaps
          - endpoints
          - nodes
          - pods
          - secrets
        verbs:
          - list
          - watch
      - apiGroups:
          - ""
        resources:
          - nodes
        verbs:
          - get
      - apiGroups:
          - ""
        resources:
          - services
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - "extensions"
        resources:
          - ingresses
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - ""
        resources:
            - events
        verbs:
            - create
            - patch
      - apiGroups:
          - "extensions"
        resources:
          - ingresses/status
        verbs:
          - update
    ---
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: Role
    metadata:
      name: nginx-ingress-role
      namespace: kube-system
    rules:
      - apiGroups:
          - ""
        resources:
          - configmaps
          - pods
          - secrets
          - namespaces
        verbs:
          - get
      - apiGroups:
          - ""
        resources:
          - configmaps
        resourceNames:
          # Defaults to "<election-id>-<ingress-class>"
          # Here: "<ingress-controller-leader>-<nginx>"
          # This has to be adapted if you change either parameter
          # when launching the nginx-ingress-controller.
          - "ingress-controller-leader-nginx"
        verbs:
          - get
          - update
      - apiGroups:
          - ""
        resources:
          - configmaps
        verbs:
          - create
      - apiGroups:
          - ""
        resources:
          - endpoints
        verbs:
          - get
          - create
          - update
    ---
    ## kube-lego
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    metadata:
      name: kube-lego
    rules:
    - apiGroups:
      - extensions
      resources:
      - ingresses
      verbs:
      - list
      - get
      - create
      - update
      - delete
      - watch
    - apiGroups:
      - ""
      resources:
      - endpoints
      - services
      - secrets
      verbs:
      - get
      - create
      - update
- path: /srv/rbac-bindings.yaml
  owner: root
  permissions: 0644
  content: |
    ## User
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: giantswarm-admin
    subjects:
    - kind: User
      name: ${API_DOMAIN_NAME}
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: cluster-admin
      apiGroup: rbac.authorization.k8s.io
    ---
    ## Worker
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: kubelet
    subjects:
    - kind: User
      name: ${API_DOMAIN_NAME}
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: system:node
      apiGroup: rbac.authorization.k8s.io
    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: proxy
    subjects:
    - kind: User
      name: ${API_DOMAIN_NAME}
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: system:node-proxier
      apiGroup: rbac.authorization.k8s.io
    ---
    ## Master
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: kube-controller-manager
    subjects:
    - kind: User
      name: ${API_DOMAIN_NAME}
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: system:kube-controller-manager
      apiGroup: rbac.authorization.k8s.io
    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: kube-scheduler
    subjects:
    - kind: User
      name: ${API_DOMAIN_NAME}
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: ClusterRole
      name: system:kube-scheduler
      apiGroup: rbac.authorization.k8s.io
    ---
    ## Calico
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: calico-node
    subjects:
    - kind: ServiceAccount
      name: calico-node
      namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: calico-node
      apiGroup: rbac.authorization.k8s.io
    ---
    ## DNS
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      annotations:
        rbac.authorization.kubernetes.io/autoupdate: "true"
      labels:
        kubernetes.io/bootstrapping: rbac-defaults
      name: system:coredns
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:coredns
    subjects:
    - kind: ServiceAccount
      name: coredns
      namespace: kube-system
    ---
    ## IC
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: nginx-ingress-controller
    subjects:
    - kind: ServiceAccount
      name: nginx-ingress-controller
      namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: nginx-ingress-controller
      apiGroup: rbac.authorization.k8s.io
    ---
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: nginx-ingress-controller
      namespace: kube-system
    subjects:
    - kind: ServiceAccount
      name: nginx-ingress-controller
      namespace: kube-system
    roleRef:
      kind: Role
      name: nginx-ingress-role
      apiGroup: rbac.authorization.k8s.io
    ---
    ## kube-lego
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1beta1
    metadata:
      name: kube-lego
    subjects:
    - kind: ServiceAccount
      name: kube-lego
      namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: kube-lego
      apiGroup: rbac.authorization.k8s.io
- path: /srv/psp-policies.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: extensions/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: privileged
    spec:
      fsGroup:
        rule: RunAsAny
      privileged: true
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
      - '*'
      hostPID: true
      hostIPC: true
      hostNetwork: true
      hostPorts:
      - min: 1
        max: 65536
    ---
    apiVersion: extensions/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: restricted
    spec:
      privileged: false
      fsGroup:
        rule: RunAsAny
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
      - 'emptyDir'
      - 'secret'
      - 'downwardAPI'
      - 'configMap'
      - 'persistentVolumeClaim'
      - 'projected'
      hostPID: false
      hostIPC: false
      hostNetwork: false
- path: /srv/psp-roles.yaml
  owner: root
  permissions: 0644
  content: |
    # restrictedPSP grants access to use
    # the restricted PSP.
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    metadata:
      name: restricted-psp-user
    rules: 
    - apiGroups:
      - extensions
      resources:
      - podsecuritypolicies
      resourceNames:
      - restricted
      verbs:
      - use
    ---
    # privilegedPSP grants access to use the privileged
    # PSP.
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRole
    metadata:
      name: privileged-psp-user
    rules: 
    - apiGroups:
      - extensions
      resources:
      - podsecuritypolicies
      resourceNames:
      - privileged
      verbs:
      - use
- path: /srv/psp-bindings.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
        name: privileged-psp-users
    subjects:
    - kind: ServiceAccount
      name: calico-node
      namespace: kube-system
    - kind: ServiceAccount
      name: coredns
      namespace: kube-system
    - kind: ServiceAccount
      name: nginx-ingress-controller
      namespace: kube-system
    - kind: ServiceAccount
      name: kube-lego
      namespace: kube-system
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: privileged-psp-user
    ---
    # grants the restricted PSP role to
    # the all authenticated users.
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
        name: restricted-psp-users
    subjects:
    - kind: Group
      apiGroup: rbac.authorization.k8s.io
      name: system:authenticated
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: restricted-psp-user
- path: /opt/wait-for-domains
  permissions: 0544
  content: |
      #!/bin/bash
      domains="${ETCD_DOMAIN_NAME} ${API_DOMAIN_NAME} ${VAULT_DOMAIN_NAME}"

      for domain in $domains; do
        until nslookup $domain; do
            echo "Waiting for domain $domain to be available"
            sleep 5
        done

        echo "Successfully resolved domain $domain"
      done
- path: /opt/k8s-addons
  permissions: 0544
  content: |
      #!/bin/bash
      KUBECTL=quay.io/giantswarm/docker-kubectl:f51f93c30d27927d2b33122994c0929b3e6f2432

      /usr/bin/docker pull $KUBECTL

      # wait for healthy master
      while [ "$(/usr/bin/docker run --net=host --rm $KUBECTL get cs | grep Healthy | wc -l)" -ne "3" ]; do sleep 1 && echo 'Waiting for healthy k8s'; done

      # apply Security bootstrap (RBAC and PSP)
      SECURITY_FILES="rbac-roles.yaml\
        rbac-bindings.yaml\
        psp-policies.yaml\
        psp-roles.yaml\
        psp-bindings.yaml"

      for manifest in $SECURITY_FILES
      do
          while
              /usr/bin/docker run --net=host --rm -v /srv:/srv $KUBECTL apply -f /srv/$manifest
              [ "$?" -ne "0" ]
          do
              echo "failed to apply /srv/$manifest, retrying in 5 sec"
              sleep 5s
          done
      done

      # apply calico CNI
      CALICO_FILE="calico-all.yaml"

      while
          /usr/bin/docker run --net=host --rm -v /srv:/srv $KUBECTL apply -f /srv/$CALICO_FILE
          [ "$?" -ne "0" ]
      do
          echo "failed to apply /src/$CALICO_FILE, retrying in 5 sec"
          sleep 5s
      done

      # wait for healthy calico - we check for pods - desired vs ready
      while
          # result of this is 'eval [ "$DESIRED_POD_COUNT" -eq "$READY_POD_COUNT" ]'
          /usr/bin/docker run --net=host --rm -v /etc/kubernetes:/etc/kubernetes $KUBECTL -n kube-system  get ds calico-node 2>/dev/null >/dev/null
          RET_CODE_1=$?
          eval $(/usr/bin/docker run --net=host --rm $KUBECTL -n kube-system get ds calico-node | tail -1 | awk '{print "[ \"" $2"\" -eq \""$4"\" ] "}')
          RET_CODE_2=$?
          [ "$RET_CODE_1" -ne "0" ] || [ "$RET_CODE_2" -ne "0" ]
      do
          echo "Waiting for calico to be ready . . "
          sleep 3s
      done

      # apply k8s addons
      MANIFESTS="default-storage-class.yaml coredns-all.yaml default-backend-dep.yaml default-backend-svc.yaml ingress-controller-cm.yaml ingress-controller-sa.yaml ingress-controller-dep.yaml ingress-controller-svc.yaml kube-lego-cm.yaml kube-lego-sa.yaml kube-lego-dep.yaml"

      for manifest in $MANIFESTS
      do
          while
              /usr/bin/docker run --net=host --rm -v /srv:/srv $KUBECTL apply -f /srv/$manifest
              [ "$?" -ne "0" ]
          do
              echo "failed to apply /src/$manifest, retrying in 5 sec"
              sleep 5s
          done
      done
      echo "Addons successfully installed"

- path: /etc/kubernetes/config/azure.yaml
  owner: root
  permissions: 0600
  content: |
    cloud: ${AZURE_CLOUD}
    tenantId: ${AZURE_SP_TENANTID}
    subscriptionId: ${AZURE_SP_SUBSCRIPTIONID}
    aadClientId: ${AZURE_SP_AADCLIENTID}
    aadClientSecret: ${AZURE_SP_AADCLIENTSECRET}
    resourceGroup: ${AZURE_RESOURCEGROUP}
    location: ${AZURE_LOCATION}
    subnetName: ${AZURE_SUBNETNAME}
    securityGroupName: ${AZURE_SECGROUPNAME}
    vnetName: ${AZURE_VNETNAME}
    routeTableName: ${AZURE_ROUTETABLE}

- path: /etc/kubernetes/config/proxy-kubeconfig.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Config
    users:
    - name: proxy
      user:
        client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
        client-key: /etc/kubernetes/ssl/apiserver-key.pem
    clusters:
    - name: local
      cluster:
        certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
        server: https://127.0.0.1
    contexts:
    - context:
        cluster: local
        user: proxy
      name: service-account-context
    current-context: service-account-context
- path: /etc/kubernetes/config/kubelet-kubeconfig.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Config
    users:
    - name: kubelet
      user:
        client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
        client-key: /etc/kubernetes/ssl/apiserver-key.pem
    clusters:
    - name: local
      cluster:
        certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
        server: https://127.0.0.1
    contexts:
    - context:
        cluster: local
        user: kubelet
      name: service-account-context
    current-context: service-account-context
- path: /etc/kubernetes/config/controller-manager-kubeconfig.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Config
    users:
    - name: controller-manager
      user:
        client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
        client-key: /etc/kubernetes/ssl/apiserver-key.pem
    clusters:
    - name: local
      cluster:
        certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
        server: https://127.0.0.1
    contexts:
    - context:
        cluster: local
        user: controller-manager
      name: service-account-context
    current-context: service-account-context
- path: /etc/kubernetes/config/scheduler-kubeconfig.yaml
  owner: root
  permissions: 0644
  content: |
    apiVersion: v1
    kind: Config
    users:
    - name: scheduler
      user:
        client-certificate: /etc/kubernetes/ssl/apiserver-crt.pem
        client-key: /etc/kubernetes/ssl/apiserver-key.pem
    clusters:
    - name: local
      cluster:
        certificate-authority: /etc/kubernetes/ssl/apiserver-ca.pem
        server: https://127.0.0.1
    contexts:
    - context:
        cluster: local
        user: scheduler
      name: service-account-context
    current-context: service-account-context

- path: /etc/tokens/node
  owner: root
  permissions: 0400
  content: |
    VAULT_TOKEN=${G8S_VAULT_TOKEN}

- path: /etc/ssh/sshd_config
  owner: root
  permissions: 0600
  content: |
    # Use most defaults for sshd configuration.
    UsePrivilegeSeparation sandbox
    Subsystem sftp internal-sftp
    ClientAliveInterval 180
    UseDNS no
    UsePAM yes
    PrintLastLog no # handled by PAM
    PrintMotd no # handled by PAM
    # Non defaults (#100)
    ClientAliveCountMax 2
    PasswordAuthentication no

- path: /opt/get-ca.sh
  owner: root
  permissions: 0770
  content: |
    #!/bin/bash

    if [ -z "$1" ] || [ -z "$2" ]
    then
            echo "Insufficient number of args"
            echo "$0 <ive_ip_address>:<port> <output_file>"
            exit
    fi
    echo Connecting to $1
    echo -n | openssl s_client -showcerts -connect $1 2>err.txt 1>out.txt
    if [ "$?" -ne "0" ]
    then
            cat err.txt
            exit
    fi
    echo -n Generating Certificate
    grep -in "\-----.*CERTIFICATE-----"  out.txt | cut -f 1 -d ":" 1> out1.txt
    let start_line=`tail -n 2 out1.txt | head -n 1`
    let end_line=`tail -n 1 out1.txt`
    if [ -z "$start_line" ]
    then
            echo "error"
            exit
    fi
    let nof_lines=$end_line-$start_line+1
    #echo "from $start_line to $end_line total lines $nof_lines"
    echo -n " .... "
    head -n $end_line out.txt | tail -n $nof_lines 1> out1.txt
    openssl x509 -in out1.txt -outform pem -out $2
    echo done.
    rm out.txt out1.txt err.txt

- path: /etc/sysctl.d/hardening.conf
  owner: root
  permissions: 0600
  content: |
    kernel.kptr_restrict = 2
    kernel.sysrq = 0
    net.ipv4.conf.all.send_redirects = 0
    net.ipv4.conf.default.accept_redirects = 0
    net.ipv4.tcp_timestamps = 0
    net.ipv6.conf.all.accept_redirects = 0
    net.ipv6.conf.default.accept_redirects = 0

- path: /etc/audit/rules.d/10-docker.rules
  owner: root
  permissions: 0644
  content: |
    -w /usr/bin/docker -k docker
    -w /var/lib/docker -k docker
    -w /etc/docker -k docker
    -w /etc/systemd/system/docker.service.d/10-giantswarm-extra-args.conf -k docker
    -w /etc/systemd/system/docker.service.d/01-wait-docker.conf -k docker
    -w /usr/lib/systemd/system/docker.service -k docker
    -w /usr/lib/systemd/system/docker.socket -k docker

- path: /etc/systemd/system/audit-rules.service.d/10-Wait-For-Docker.conf
  owner: root
  permissions: 0644
  content: |
    [Service]
    ExecStartPre=/bin/bash -c "while [ ! -f /etc/audit/rules.d/10-docker.rules ]; do echo 'Waiting for /etc/audit/rules.d/10-docker.rules to be written' && sleep 1; done"

- path: /etc/udev/rules.d/99-systemd.rules
  owner: root
  permissions: 0644
  content: |
    SUBSYSTEM=="block", KERNEL=="sdd", TAG+="systemd"

coreos:
  units:
  - name: format-etcd-disk.service
    command: start
    content: |
      [Unit]
      Description=Formats the disk drive
      Requires=dev-sdd.device
      After=dev-sdd.device
      [Service]
      Type=oneshot
      RemainAfterExit=yes
      Environment="LABEL=var-lib-etcd"
      Environment="DEV=/dev/sdd"
      # Do not wipe the disk if it's already being used, so the etcd data is persistent across reboot.
      ExecStart=-/bin/bash -c "if ! findfs LABEL=$LABEL > /tmp/label.$LABEL; then wipefs -a -f $DEV && mkfs.ext4 -T news -F -L $LABEL $DEV && echo wiped; fi"
  - name: var-lib-etcd.mount
    command: start
    content: |
      [Unit]
      Description=Mount disk to /var/lib/etcd
      Requires=format-etcd-disk.service
      After=format-etcd-disk.service
      Before=etcd3.service
      [Mount]
      What=/dev/sdd
      Where=/var/lib/etcd
      Type=ext4
  - name: format-docker-disk.service
    command: start
    content: |
      [Unit]
      Description=Formats the disk drive
      [Service]
      Type=oneshot
      RemainAfterExit=yes
      Environment="LABEL=docker"
      Environment="DEV=/dev/sdc"
      # Do not wipe the disk if it's already being used, so the etcd data is persistent across reboot.
      ExecStart=-/bin/bash -c "if ! findfs LABEL=$LABEL > /tmp/label.$LABEL; then wipefs -a -f $DEV && mkfs.xfs  -L $LABEL $DEV && echo wiped; fi"
  - name: var-lib-docker.mount
    command: start
    content: |
      [Unit]
      Description=Mount disk to /var/lib/docker
      Requires=format-docker-disk.service
      After=format-docker-disk.service
      Before=docker.service
      [Mount]
      What=/dev/sdc
      Where=/var/lib/docker
      Type=xfs
  - name: wait-for-domains.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=Wait for etcd, k8s API and vault  domains to be available

      [Service]
      Type=oneshot
      ExecStart=/opt/wait-for-domains

      [Install]
      WantedBy=multi-user.target
  - name: os-hardening.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=Apply os hardening

      [Service]
      Type=oneshot
      ExecStartPre=-/bin/bash -c "gpasswd -d core rkt; gpasswd -d core docker; gpasswd -d core wheel"
      ExecStartPre=/bin/bash -c "until [ -f '/etc/sysctl.d/hardening.conf' ]; do echo Waiting for sysctl file; sleep 1s;done;"
      ExecStart=/usr/sbin/sysctl -p /etc/sysctl.d/hardening.conf

      [Install]
      WantedBy=multi-user.target
  - name: docker.service
    enable: true
    command: start
    drop-ins:
    - name: 10-giantswarm-extra-args.conf
      content: |
        [Unit]
        Requires=var-lib-docker.mount
        After=var-lib-docker.mount

        [Service]
        Environment="DOCKER_CGROUPS=--exec-opt native.cgroupdriver=cgroupfs --log-opt max-size=50m --log-opt max-file=2 --log-opt labels=io.kubernetes.container.hash,io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace,io.kubernetes.pod.uid"
        Environment="DOCKER_OPT_BIP=--bip=${DOCKER_CIDR}"
        Environment="DOCKER_OPTS=--live-restore --userland-proxy=false --icc=false"
  - name: k8s-setup-network-env.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-setup-network-env Service
      Wants=network.target docker.service
      After=network.target docker.service

      [Service]
      Type=oneshot
      RemainAfterExit=yes
      TimeoutStartSec=0
      Environment="IMAGE=quay.io/giantswarm/k8s-setup-network-environment:1f4ffc52095ac368847ce3428ea99b257003d9b9"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/mkdir -p /opt/bin/
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStart=/usr/bin/docker run --rm --net=host -v /etc:/etc --name $NAME $IMAGE
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: etcd2.service
    command: stop
    enable: false
    mask: true
  - name: flanneld.service
    enable: false
    command: stop
    mask: true
  - name: fleet.service
    enable: false
    command: stop
    mask: true
  - name: fleet.socket
    enable: false
    mask: true
    command: stop
  - name: get-vault-ca.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=get vault-ca into trusted certs
      Before=calico-certs.service api-certs.service etcd3-certs.service
      After=wait-for-domains.service

      [Service]
      Type=oneshot
      ExecStartPre=/opt/get-ca.sh ${VAULT_DOMAIN_NAME}:443 /etc/ssl/certs/gs-ca.pem
      ExecStart=/sbin/update-ca-certificates
      RemainAfterExit=yes
  - name: calico-certs.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=calico-certs
      Requires=docker.service
      After=docker.service

      [Service]
      EnvironmentFile=/etc/environment
      EnvironmentFile=/etc/network-environment
      EnvironmentFile=/etc/tokens/node
      Type=oneshot
      RemainAfterExit=yes
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/ssl/calico/
      ExecStart=/usr/bin/docker run \
      --net=host \
      -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \
      -v /etc/kubernetes/ssl/calico/:/etc/kubernetes/ssl/calico/ \
      quay.io/giantswarm/certctl:b07d0913d5cb369a6b605394bdd4be4633451be9 \
      issue \
      --vault-addr=https://${VAULT_DOMAIN_NAME} \
      --vault-token=$${VAULT_TOKEN} \
      --cluster-id=g8s \
      --common-name=${ETCD_DOMAIN_NAME} \
      --ttl=8760h \
      --crt-file=/etc/kubernetes/ssl/calico/client-crt.pem \
      --ip-sans=127.0.0.1,${DEFAULT_IPV4} \
      --alt-names=localhost \
      --key-file=/etc/kubernetes/ssl/calico/client-key.pem \
      --ca-file=/etc/kubernetes/ssl/calico/client-ca.pem
      ExecStop=/usr/bin/rm -rf /etc/kubernetes/ssl/calico/
  - name: etcd3-certs.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=etcd3-certs
      Requires=docker.service
      After=docker.service

      [Service]
      EnvironmentFile=/etc/environment
      EnvironmentFile=/etc/network-environment
      EnvironmentFile=/etc/tokens/node
      Type=oneshot
      RemainAfterExit=yes
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/ssl/etcd/
      ExecStart=/usr/bin/docker run \
      --net=host \
      -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \
      -v /etc/kubernetes/ssl/etcd/:/etc/kubernetes/ssl/etcd/ \
      quay.io/giantswarm/certctl:b07d0913d5cb369a6b605394bdd4be4633451be9 \
      issue \
      --vault-addr=https://${VAULT_DOMAIN_NAME} \
      --vault-token=$${VAULT_TOKEN} \
      --cluster-id=g8s \
      --common-name=${ETCD_DOMAIN_NAME} \
      --ttl=8760h \
      --crt-file=/etc/kubernetes/ssl/etcd/server-crt.pem \
      --ip-sans=127.0.0.1,${DEFAULT_IPV4} \
      --alt-names=localhost \
      --key-file=/etc/kubernetes/ssl/etcd/server-key.pem \
      --ca-file=/etc/kubernetes/ssl/etcd/server-ca.pem
      ExecStartPost=/bin/cp /etc/kubernetes/ssl/etcd/server-crt.pem /etc/kubernetes/ssl/etcd/client-crt.pem
      ExecStartPost=/bin/cp /etc/kubernetes/ssl/etcd/server-ca.pem /etc/kubernetes/ssl/etcd/client-ca.pem
      ExecStartPost=/bin/cp /etc/kubernetes/ssl/etcd/server-key.pem /etc/kubernetes/ssl/etcd/client-key.pem
      ExecStop=/usr/bin/rm -rf /etc/kubernetes/ssl/etcd/
  - name: etcd3.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=etcd3
      Requires=k8s-setup-network-env.service var-lib-etcd.mount
      After=k8s-setup-network-env.service etcd3-certs.service calico-certs.service var-lib-etcd.mount
      Conflicts=etcd.service etcd2.service
      StartLimitIntervalSec=0

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      LimitNOFILE=40000
      Environment=IMAGE=quay.io/coreos/etcd:v3.3.1
      Environment=NAME=%p.service
      EnvironmentFile=/etc/network-environment
      ExecStartPre=-/usr/bin/docker stop  $NAME
      ExecStartPre=-/usr/bin/docker rm  $NAME
      ExecStartPre=-/usr/bin/docker pull $IMAGE
      ExecStartPre=/bin/bash -c "while [ ! -f /etc/kubernetes/ssl/etcd/server-ca.pem ]; do echo 'Waiting for /etc/kubernetes/ssl/etcd/server-ca.pem to be written' && sleep 1; done"
      ExecStartPre=/bin/bash -c "while [ ! -f /etc/kubernetes/ssl/etcd/server-crt.pem ]; do echo 'Waiting for /etc/kubernetes/ssl/etcd/server-crt.pem to be written' && sleep 1; done"
      ExecStartPre=/bin/bash -c "while [ ! -f /etc/kubernetes/ssl/etcd/server-key.pem ]; do echo 'Waiting for /etc/kubernetes/ssl/etcd/server-key.pem to be written' && sleep 1; done"
      ExecStart=/usr/bin/docker run \
          -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \
          -v /etc/kubernetes/ssl/etcd/:/etc/etcd \
          -v /var/lib/etcd/:/var/lib/etcd  \
          --net=host  \
          --name $NAME \
          $IMAGE \
          etcd \
          --name etcd0 \
          --trusted-ca-file /etc/etcd/server-ca.pem \
          --cert-file /etc/etcd/server-crt.pem \
          --key-file /etc/etcd/server-key.pem \
          --client-cert-auth=true \
          --peer-trusted-ca-file /etc/etcd/server-ca.pem \
          --peer-cert-file /etc/etcd/server-crt.pem \
          --peer-key-file /etc/etcd/server-key.pem \
          --peer-client-cert-auth=true \
          --advertise-client-urls=https://${ETCD_DOMAIN_NAME}:2379 \
          --initial-advertise-peer-urls=https://127.0.0.1:2380 \
          --listen-client-urls=https://0.0.0.0:2379 \
          --listen-peer-urls=https://${DEFAULT_IPV4}:2380 \
          --initial-cluster-token k8s-etcd-cluster \
          --initial-cluster etcd0=https://127.0.0.1:2380 \
          --initial-cluster-state new \
          --data-dir=/var/lib/etcd \
          --enable-v2

      [Install]
      WantedBy=multi-user.target
  - name: etcd3-defragmentation.service
    enable: false
    content: |
      [Unit]
      Description=etcd defragmentation job
      After=docker.service etcd3.service
      Requires=docker.service etcd3.service

      [Service]
      Type=oneshot
      EnvironmentFile=/etc/environment
      Environment=IMAGE=quay.io/coreos/etcd:v3.3.1
      Environment=NAME=%p.service
      ExecStartPre=-/usr/bin/docker stop  $NAME
      ExecStartPre=-/usr/bin/docker rm  $NAME
      ExecStartPre=-/usr/bin/docker pull $IMAGE
      ExecStart=/usr/bin/docker run \
        -v /etc/kubernetes/ssl/etcd/:/etc/etcd \
        --net=host  \
        -e ETCDCTL_API=3 \
        --name $NAME \
        $IMAGE \
        etcdctl \
        --endpoints https://127.0.0.1:2379 \
        --cacert /etc/etcd/server-ca.pem \
        --cert /etc/etcd/server-crt.pem \
        --key /etc/etcd/server-key.pem \
        defrag

      [Install]
      WantedBy=multi-user.target
  - name: etcd3-defragmentation.timer
    enable: true
    command: start
    content: |
      [Unit]
      Description=Execute etcd3-defragmentation every day at 3.30AM UTC

      [Timer]
      OnCalendar=*-*-* 03:30:00 UTC

      [Install]
      WantedBy=multi-user.target
  - name: k8s-proxy.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-proxy
      StartLimitIntervalSec=0

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      EnvironmentFile=/etc/network-environment
      Environment="IMAGE=quay.io/giantswarm/hyperkube:v1.10.0"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStartPre=/bin/sh -c "while ! curl --output /dev/null --silent --head --fail --cacert /etc/kubernetes/ssl/apiserver-ca.pem --cert /etc/kubernetes/ssl/apiserver-crt.pem --key /etc/kubernetes/ssl/apiserver-key.pem https://127.0.0.1; do sleep 1 && echo 'Waiting for master'; done"
      ExecStart=/bin/sh -c "/usr/bin/docker run --rm --net=host --privileged=true \
      --name $NAME \
      -v /usr/share/ca-certificates:/etc/ssl/certs \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      $IMAGE \
      /hyperkube proxy \
      --master=https://127.0.0.1 \
      --proxy-mode=iptables \
      --logtostderr=true \
      --kubeconfig=/etc/kubernetes/config/proxy-kubeconfig.yaml \
      --v=2"
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: k8s-kubelet.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-kubelet
      StartLimitIntervalSec=0
      After=k8s-addons.service k8s-api-server.service

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      EnvironmentFile=/etc/network-environment
      Environment="IMAGE=quay.io/giantswarm/hyperkube:v1.10.0"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStart=/bin/sh -c "/usr/bin/docker run --rm --pid=host --net=host --privileged=true \
      -v /:/rootfs:ro,rshared \
      -v /sys:/sys:ro \
      -v /dev:/dev:rw \
      -v /run/calico/:/run/calico/:rw \
      -v /run/docker/:/run/docker/:rw \
      -v /run/docker.sock:/run/docker.sock:rw \
      -v /var/log:/var/log:rw \
      -v /usr/lib/os-release:/etc/os-release \
      -v /usr/share/ca-certificates/:/etc/ssl/certs \
      -v /var/lib/docker/:/var/lib/docker:rw,rshared \
      -v /var/lib/kubelet/:/var/lib/kubelet:rw,rshared \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      -v /etc/cni/net.d/:/etc/cni/net.d/ \
      -v /opt/cni/bin/:/opt/cni/bin/ \
      -e ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/etcd/server-ca.pem \
      -e ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd/server-crt.pem \
      -e ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd/server-key.pem \
      --name $NAME \
      $IMAGE \
      /hyperkube kubelet \
      --address=${DEFAULT_IPV4} \
      --port=10250 \
      --node-ip=${DEFAULT_IPV4} \
      --containerized \
      --enable-server \
      --logtostderr=true \
      --cloud-provider=azure \
      --cloud-config=/etc/kubernetes/config/azure.yaml \
      --machine-id-file=/rootfs/etc/machine-id \
      --cadvisor-port=4194 \
      --healthz-bind-address=${DEFAULT_IPV4} \
      --healthz-port=10248 \
      --cluster-dns=${K8S_DNS_IP} \
      --cluster-domain=cluster.local \
      --network-plugin=cni \
      --feature-gates=ExpandPersistentVolumes=true \
      --register-node=true \
      --register-with-taints=node-role.kubernetes.io/master=:NoSchedule \
      --allow-privileged=true \
      --kubeconfig=/etc/kubernetes/config/kubelet-kubeconfig.yaml \
      --node-labels="node-role.kubernetes.io/master,role=master,kubernetes.io/hostname=$HOSTNAME,ip=${DEFAULT_IPV4}," \
      --v=2"
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: update-engine.service
    enable: false
    command: stop
    mask: true
  - name: locksmithd.service
    enable: false
    command: stop
    mask: true
  - name: systemd-networkd-wait-online.service
    enable: false
    command: stop
    mask: true
  - name: api-certs.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=api-certs
      Requires=docker.service
      After=docker.service

      [Service]
      EnvironmentFile=/etc/environment
      EnvironmentFile=/etc/network-environment
      EnvironmentFile=/etc/tokens/node
      Environment=VAULT_ADDR=https://${VAULT_DOMAIN_NAME}
      Type=oneshot
      RemainAfterExit=yes
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/ssl/
      ExecStartPre=/bin/bash -c 'export rsa_key=$(docker run --rm -i -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt --net host --privileged=true -e VAULT_ADDR -e VAULT_TOKEN giantswarm/docker-vault:0.1.0 read -field=key secret/g8s_sa_sign_key 2>/dev/null); echo -e "-----BEGIN RSA PRIVATE KEY-----\n$rsa_key\n-----END RSA PRIVATE KEY-----" > /etc/kubernetes/ssl/service-account-key.pem'
      ExecStart=/usr/bin/docker run \
      --net=host \
      -v /etc/ssl/certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      quay.io/giantswarm/certctl:b07d0913d5cb369a6b605394bdd4be4633451be9 \
      issue \
      --vault-addr=$${VAULT_ADDR} \
      --vault-token=$${VAULT_TOKEN} \
      --cluster-id=g8s \
      --common-name=${API_DOMAIN_NAME} \
      --ttl=8760h \
      --crt-file=/etc/kubernetes/ssl/apiserver-crt.pem \
      --ip-sans=127.0.0.1,${DEFAULT_IPV4},${K8S_API_IP} \
      --alt-names=localhost,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.local\
      --key-file=/etc/kubernetes/ssl/apiserver-key.pem \
      --ca-file=/etc/kubernetes/ssl/apiserver-ca.pem
  - name: k8s-api-server.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-api-server
      StartLimitIntervalSec=0
      After=docker.service etcd3.service api-certs.service
      Requires=api-certs.service

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      EnvironmentFile=/etc/network-environment
      Environment="IMAGE=quay.io/giantswarm/hyperkube:v1.10.0"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStart=/usr/bin/docker run --rm --name $NAME --net=host \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      -v /var/log/apiserver/:/var/log/apiserver \
      $IMAGE \
      /hyperkube apiserver \
      --allow-privileged=true \
      --insecure-bind-address=127.0.0.1 \
      --insecure-port=8080 \
      --kubelet-https=true \
      --secure-port=443 \
      --cloud-provider=azure \
      --cloud-config=/etc/kubernetes/config/azure.yaml \
      --bind-address=0.0.0.0 \
      --etcd-prefix=giantswarm.io \
      --authorization-mode=RBAC \
      --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,PodSecurityPolicy,PersistentVolumeClaimResize,DefaultStorageClass \
      --service-cluster-ip-range=${K8S_SERVICE_CIDR} \
      --etcd-servers=https://${ETCD_DOMAIN_NAME}:2379 \
      --etcd-cafile=/etc/kubernetes/ssl/etcd/server-ca.pem \
      --etcd-certfile=/etc/kubernetes/ssl/etcd/server-crt.pem \
      --etcd-keyfile=/etc/kubernetes/ssl/etcd/server-key.pem \
      --advertise-address=${DEFAULT_IPV4} \
      --runtime-config=api/all=true \
      --logtostderr=true \
      --profiling=false \
      --repair-malformed-updates=false \
      --service-account-lookup=true \
      --tls-cert-file=/etc/kubernetes/ssl/apiserver-crt.pem \
      --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem \
      --client-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem \
      --service-account-key-file=/etc/kubernetes/ssl/service-account-key.pem \
      --feature-gates=AdvancedAuditing=false,ExpandPersistentVolumes=true \
      --audit-log-path=/var/log/apiserver/audit.log \
      --audit-log-maxage=30 \
      --audit-log-maxbackup=30 \
      --audit-log-maxsize=100
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: k8s-controller-manager.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-controller-manager Service
      StartLimitIntervalSec=0

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      EnvironmentFile=/etc/network-environment
      Environment="IMAGE=quay.io/giantswarm/hyperkube:v1.10.0"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStart=/usr/bin/docker run --rm --net=host --name $NAME \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      -v /etc/kubernetes/secrets/token_sign_key.pem:/etc/kubernetes/secrets/token_sign_key.pem \
      $IMAGE \
      /hyperkube controller-manager \
      --master=https://127.0.0.1:443 \
      --logtostderr=true \
      --v=2 \
      --cloud-provider=azure \
      --cloud-config=/etc/kubernetes/config/azure.yaml \
      --allocate-node-cidrs=true \
      --cluster-cidr ${POD_CIDR} \
      --profiling=false \
      --terminated-pod-gc-threshold=10 \
      --use-service-account-credentials=true \
      --feature-gates=ExpandPersistentVolumes=true \
      --kubeconfig=/etc/kubernetes/config/controller-manager-kubeconfig.yaml \
      --root-ca-file=/etc/kubernetes/ssl/apiserver-ca.pem \
      --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: k8s-scheduler.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=k8s-scheduler Service
      StartLimitIntervalSec=0

      [Service]
      Restart=always
      RestartSec=0
      TimeoutStopSec=10
      EnvironmentFile=/etc/network-environment
      Environment="IMAGE=quay.io/giantswarm/hyperkube:v1.10.0"
      Environment="NAME=%p.service"
      Environment="NETWORK_CONFIG_CONTAINER="
      ExecStartPre=/usr/bin/docker pull $IMAGE
      ExecStartPre=-/usr/bin/docker stop -t 10 $NAME
      ExecStartPre=-/usr/bin/docker rm -f $NAME
      ExecStart=/usr/bin/docker run --rm --net=host --name $NAME \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      $IMAGE \
      /hyperkube scheduler \
      --master=https://127.0.0.1:443 \
      --logtostderr=true \
      --v=2 \
      --profiling=false \
      --kubeconfig=/etc/kubernetes/config/scheduler-kubeconfig.yaml
      ExecStop=-/usr/bin/docker stop -t 10 $NAME
      ExecStopPost=-/usr/bin/docker rm -f $NAME
  - name: k8s-addons.service
    enable: true
    command: start
    content: |
      [Unit]
      Description=Kubernetes Addons
      Wants=k8s-api-server.service
      After=k8s-api-server.service
      [Service]
      Type=oneshot
      EnvironmentFile=/etc/network-environment
      ExecStart=/opt/k8s-addons
      [Install]
      WantedBy=multi-user.target
  update:
    reboot-strategy: off
